{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bregman_kit.BregmanClustering import BregmanHardClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bregman_kit.utils import squared_euclidean, kl_divergence, itakura_saito\n",
    "from sklearn.metrics import adjusted_rand_score, mutual_info_score\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06738a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ikbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ikbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Ikbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ikbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    tokens = [word for word in tokens if not word.isdigit()]\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4766996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_from_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n",
    "    \n",
    "    all_texts = []\n",
    "    all_filenames = []\n",
    "    \n",
    "    txt_files = glob.glob(os.path.join(folder_path, \"**\", \"*.txt\"), recursive=True)\n",
    "    \n",
    "    for file_path in txt_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                all_texts.append(preprocess_text(text))\n",
    "                all_filenames.append(os.path.basename(file_path))\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                    text = file.read()\n",
    "                    all_texts.append(text)\n",
    "                    all_filenames.append(os.path.basename(file_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file: {file_path} with both encodings. Error: {e}\")\n",
    "    return all_texts, all_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20de22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for i, cat in enumerate(categories):\n",
    "    t, _ = read_texts_from_folder(f\"News Articles/{cat}/\")\n",
    "    texts += t\n",
    "    labels += [i] * len(t)\n",
    "    \n",
    "data = pd.DataFrame({\n",
    "    'text' : texts,\n",
    "    'label' : labels\n",
    "})\n",
    "\n",
    "data = data.sample(frac=1, random_state=42)\n",
    "\n",
    "texts = data.text.to_numpy()\n",
    "labels = data.label.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e6af27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_vectors(texts, max_features=None, min_df=.1, max_df=.85, ngram_range=(1, 1)):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "        ngram_range=ngram_range\n",
    "    )\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "    return tfidf_matrix, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "375403db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_matrix, vectorizer = create_tfidf_vectors(texts)\n",
    "tfidf = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "749d2078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 262)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a881922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared Euclidean\n",
      "Silhouette Score: 0.04514586664696469\n",
      "Adjusted Rand Score: 0.4887570874702466\n",
      "Mutual Inforamtion Score: 0.8468344042869949\n",
      "Davis-Boildin Index: 4.6077540970445785\n"
     ]
    }
   ],
   "source": [
    "#Squared Eucliedan (classical kmeans)\n",
    "model = BregmanHardClustering(5, squared_euclidean, random_state=42)\n",
    "model.fit(tfidf)\n",
    "clusters = model.predict(tfidf)\n",
    "print(\"Squared Euclidean\")\n",
    "try:\n",
    "    print(f\"Silhouette Score: {silhouette_score(tfidf, clusters)}\")\n",
    "    print(f\"Adjusted Rand Score: {adjusted_rand_score(labels, clusters)}\")\n",
    "    print(f\"Mutual Inforamtion Score: {mutual_info_score(labels, clusters)}\")\n",
    "    print(f\"Davis-Boildin Index: {davies_bouldin_score(tfidf, clusters)}\")\n",
    "except:\n",
    "    print(\"Плохая кластеризация\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01325a43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL-divergence\n",
      "Silhouette Score: 0.014997960033891972\n",
      "Adjusted Rand Score: 0.7975459224558722\n",
      "Mutual Inforamtion Score: 1.217351906693596\n",
      "Davis-Boildin Index: 4.556585739937459\n"
     ]
    }
   ],
   "source": [
    "#KL-divergence\n",
    "tfidf_smoothed = tfidf + 1e-8\n",
    "tfidf_smoothed = np.array([v / np.sum(v) for v in tfidf_smoothed])\n",
    "\n",
    "model = BregmanHardClustering(5, kl_divergence, random_state=42)\n",
    "model.fit(tfidf_smoothed)\n",
    "clusters = model.predict(tfidf_smoothed)\n",
    "print(\"KL-divergence\")\n",
    "try:\n",
    "    print(f\"Silhouette Score: {silhouette_score(tfidf_smoothed, clusters)}\")\n",
    "    print(f\"Adjusted Rand Score: {adjusted_rand_score(labels, clusters)}\")\n",
    "    print(f\"Mutual Inforamtion Score: {mutual_info_score(labels, clusters)}\")\n",
    "    print(f\"Davis-Boildin Index: {davies_bouldin_score(tfidf_smoothed, clusters)}\")\n",
    "except:\n",
    "    print(\"Плохая кластеризация\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9780b2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itakura-Saito\n",
      "Silhouette Score: -0.00509930942028737\n",
      "Adjusted Rand Score: 0.4608701392289428\n",
      "Mutual Inforamtion Score: 0.756466829776836\n",
      "Davis-Boildin Index: 5.8239727372044525\n"
     ]
    }
   ],
   "source": [
    "#Itakura-Saito\n",
    "tfidf_smoothed = tfidf + 1e-8\n",
    "tfidf_smoothed = np.array([v / np.sum(v) for v in tfidf_smoothed]) * 10\n",
    "\n",
    "model = BregmanHardClustering(5, itakura_saito, random_state=42)\n",
    "model.fit(tfidf_smoothed)\n",
    "clusters = model.predict(tfidf_smoothed)\n",
    "print(\"Itakura-Saito\")\n",
    "try:\n",
    "    print(f\"Silhouette Score: {silhouette_score(tfidf_smoothed, clusters)}\")\n",
    "    print(f\"Adjusted Rand Score: {adjusted_rand_score(labels, clusters)}\")\n",
    "    print(f\"Mutual Inforamtion Score: {mutual_info_score(labels, clusters)}\")\n",
    "    print(f\"Davis-Boildin Index: {davies_bouldin_score(tfidf_smoothed, clusters)}\")\n",
    "except:\n",
    "    print(\"Плохая кластеризация\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemorr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
